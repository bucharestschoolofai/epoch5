{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Epoch 5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "4dGsCBYnnmOl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Epoch 5: Computer Vision"
      ]
    },
    {
      "metadata": {
        "id": "HsmSirNFnrxr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Imports and utility functions"
      ]
    },
    {
      "metadata": {
        "id": "bvLlnkzEnh0w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import os\n",
        "\n",
        "def display_activation(activations, col_size, row_size, layer_number): \n",
        "  activation = activations[layer_number] \n",
        "  activation_index = 0\n",
        "  \n",
        "  if len(activation.shape) == 2:\n",
        "    activation = activation.reshape(1, 1, activation.shape[-1])\n",
        "    plt.imshow(activation[0], cmap='gray')\n",
        "    plt.gcf().set_size_inches(21, 14)\n",
        "    return\n",
        "\n",
        "  fig, ax = plt.subplots(row_size, col_size, figsize=(row_size*5,col_size*5))\n",
        "  for row in range(0, row_size):\n",
        "    for col in range(0, col_size):\n",
        "      \n",
        "      try:\n",
        "        ax[row][col].imshow(activation[0, :, :, activation_index], cmap='gray')\n",
        "        activation_index += 1\n",
        "      except IndexError as e:\n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "utJ8z41TERSj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Classification: MNIST"
      ]
    },
    {
      "metadata": {
        "id": "ZeUCXQ7p6XS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Image processing is different from the usual classical feed-forward methods we've seen in the past. The way it differs is that images have information that displays **traslational variance**. Feed forward networks do not take this into account, so we need to adjust our algorithm to fit this type of data. \n",
        "\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/0*JLEJUX1xVlsz5Yfg)"
      ]
    },
    {
      "metadata": {
        "id": "jIQ1mlRE7JN3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can achieve this using **convolutional neural networks**. \n",
        "\n",
        "\n",
        "![](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/Images/Conv2dUsage1.png)\n",
        "\n",
        "Convolutions, in the case of image processing, consist of smaller images that slide over the input image, and the result is a modified version of the input. \n",
        "\n",
        "\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/4/4f/3D_Convolution_Animation.gif)\n",
        "\n",
        "\n",
        "A convolutional neural network is a collection of such \"smaller images\", organized in a hierarchical manner. \n",
        "\n",
        "\n",
        "![](https://ujwlkarn.files.wordpress.com/2016/08/conv_all.png?w=748)\n",
        "\n",
        "Usually, the output layers are feed-forward layers, that are used to classify the feature maps. Actually, we can put any classifier there (a SVM / RandomForest etc), and it will work just as fine. The reason we use feed-forward layers is that the network can be trained end to end using backpropagation.\n",
        "\n",
        "![](https://developer.nvidia.com/sites/default/files/pictures/2018/convolutional_neural_network.png)\n",
        "\n",
        "\n",
        "Take a look at this video to understand more of how a neural network handles images:\n",
        "\n",
        "- https://www.youtube.com/watch?v=BFdMrDOx_CM\n"
      ]
    },
    {
      "metadata": {
        "id": "5P--Vlli9YQ5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## MNIST"
      ]
    },
    {
      "metadata": {
        "id": "ECzS7M7RjP8v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-k4vlVIJG-6j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://cdn-images-1.medium.com/max/1600/1*_RLj3E4Lt8cZzlwtmcbqlA.png)"
      ]
    },
    {
      "metadata": {
        "id": "6y3AlGIWjQ_b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c24nVcYgjhzm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=(28, 28, 1)))\n",
        "\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Add more convolutional, pooling, and dense layers\n",
        "\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(32, activation='relu'))\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ULGwuRjBjkhL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "          epochs=1,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d7NqyoAm-TWM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Over 95% accuracy with only a few layers. Let's see the feature maps."
      ]
    },
    {
      "metadata": {
        "id": "4UiRiGvi-oN2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image_index = 0\n",
        "\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "activations = activation_model.predict(x_train[image_index].reshape(1, 28, 28, 1))\n",
        "\n",
        "display_activation(activations, 6, 6, 1)\n",
        "plt.title(y_train[image_index], fontsize=24)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6b7nJK-Wn0ze",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Classification: CIFAR-10 "
      ]
    },
    {
      "metadata": {
        "id": "BQIw80L5oHDe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Convolution Neural Networks are very suited for images. A convolution acts as a pattern recognition. It looks only in a small region of pixels (3x3 or 5x5). The layers now are no longer fully connected, but a neuron is connected only with a few neurons."
      ]
    },
    {
      "metadata": {
        "id": "zXDBya-joKhW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Take a look at these useful links:\n",
        "* https://medium.com/@eternalzer0dayx/demystifying-convolutional-neural-networks-ca17bdc75559\n",
        "​\n",
        "* http://cs231n.github.io/convolutional-networks/\n",
        "​\n",
        "* https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/"
      ]
    },
    {
      "metadata": {
        "id": "QRD2TyJF4UpT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://shafeentejani.github.io/assets/images/convolutional_neural_network.png)"
      ]
    },
    {
      "metadata": {
        "id": "PBG1AM3boLa9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train, y_train),(x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "x_train = x_train[:5000]\n",
        "y_train = y_train[:5000]\n",
        "x_test = x_test[:1000]\n",
        "y_test = y_test[:1000]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8kQ68AyTsOaq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll use [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset for classification. It has 10 classes of various objects. They are 32x32 colored images. Let's take a look at the first 5:"
      ]
    },
    {
      "metadata": {
        "id": "YgtPuU0xrzke",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class_names = [\n",
        "  'airplane',\n",
        "  'automobile',\n",
        "  'bird',\n",
        "  'cat',\n",
        "  'deer',\n",
        "  'dog',\n",
        "  'frog',\n",
        "  'horse',\n",
        "  'ship',\n",
        "  'truck'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j1qlUkpKrOHk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 5)\n",
        "\n",
        "for i in range(5):\n",
        "  ax[i].imshow(x_train[i])\n",
        "  ax[i].set_title(\"Class: \" + str(class_names[y_train[i][0]]))\n",
        "  ax[i].grid(False)\n",
        "fig.set_size_inches(21, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gOBWJs9soL1o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will use the [keras 2D Convolution layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [pooling layers](https://keras.io/layers/pooling/) to reduce the image size and make the network faster and less prone to overfitting."
      ]
    },
    {
      "metadata": {
        "id": "skZTFL1NpDfs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "model.add(keras.layers.Conv2D(filters=32, kernel_size=(3, 3), input_shape = (32, 32, 3), activation='relu'))\n",
        "model.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dropout(0.5))\n",
        "model.add(keras.layers.Dense(32, activation='relu'))\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GodpYWGEqMkZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "normalized_x_train = x_train / 255\n",
        "normalized_x_test = x_test / 255\n",
        "\n",
        "model.fit(normalized_x_train, y_train, epochs=5, validation_data=(normalized_x_test, y_test), batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4sq23saUouge",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Visualising the filters\n",
        "\n",
        "Remember that each layer of a convolutional network consists of a set of **filters** (i.e. small images) that are learned. We can actually visualize them to gain some intuition about how the model learns."
      ]
    },
    {
      "metadata": {
        "id": "8YS0i0v-ooVV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "activations = activation_model.predict(normalized_x_train[1].reshape(1, 32, 32, 3))\n",
        "\n",
        "display_activation(activations, 6, 6, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7RLiO3BLxF4X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use the visualization to ensure that our model learned correctly. If we see purely noise, especially in the first layer, it's a sure sign the model hasn't learned properly. On the other hand, if we see filters that are black, it means that they are redundant, and can be eliminated to make the model smaller (faster) but with the same accuracy."
      ]
    },
    {
      "metadata": {
        "id": "WATDim7Hxlb6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Autoencoder \n",
        "\n",
        "The autoencoder works a bit differently than a normal convolutional network. The main difference is that a \"bottleneck\" is present, and that the \"labels\" are actual images instead of classes. We can use an autoencoder for denoising images.\n",
        "\n",
        "Check these links to learn more:\n",
        "\n",
        "- https://www.jeremyjordan.me/autoencoders/\n",
        "\n",
        "- https://www.youtube.com/watch?v=Rdpbnd0pCiI\n",
        "\n",
        "Check out this survey to learn more about the Deep Learning evolution, in general:\n",
        "\n",
        "- https://arxiv.org/pdf/1404.7828.pdf"
      ]
    },
    {
      "metadata": {
        "id": "6fDr7rsu0hci",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-06-at-3.17.13-PM.png)"
      ]
    },
    {
      "metadata": {
        "id": "4_AXLYNxzA1n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  \n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "x_train = x_train[:1000]\n",
        "x_test = x_test[:1000]\n",
        "\n",
        "noise_factor = 0.4\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
        "\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SiCHZtX7zN_x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "n = 10\n",
        "plt.figure(figsize=(20, 2))\n",
        "for i in range(1, n + 1):\n",
        "    ax = plt.subplot(1, n, i)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H1K4HivZyMGv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![](https://www.jeremyjordan.me/content/images/2018/03/Screen-Shot-2018-03-09-at-10.20.44-AM.png)"
      ]
    },
    {
      "metadata": {
        "id": "uxsyPqAszO9R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "\n",
        "# encoder\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(28, 28, 1)))\n",
        "model.add(keras.layers.AveragePooling2D((2, 2), padding='same'))\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(keras.layers.AveragePooling2D((2, 2), padding='same'))\n",
        "\n",
        "\n",
        "# decoder\n",
        "\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(keras.layers.UpSampling2D((2, 2)))\n",
        "model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
        "model.add(keras.layers.UpSampling2D((2, 2)))\n",
        "\n",
        "# output - 1 channel for gray images\n",
        "model.add(keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'))\n",
        "         \n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hYu5bkcq3kKQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our autoencoder makes use of Upsampling2D layer, which is a \"deconvolution\". Instead of making the input smaller, it make it larger. Basically it works as a normal image resize and then a normal convolution operation.\n",
        "\n",
        "![](https://i.stack.imgur.com/YyCu2.gif)\n"
      ]
    },
    {
      "metadata": {
        "id": "6Z1Cwrp10LDZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train_noisy, x_train, epochs=1, batch_size=1, validation_data=(x_test_noisy, x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tPqxAErw0ysF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "decoded_imgs = model.predict(x_test_noisy)\n",
        "\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(1, n + 1):\n",
        "    # display original\n",
        "    ax = plt.subplot(3, n, i)\n",
        "    plt.imshow(x_test[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "    \n",
        "    \n",
        "    ax = plt.subplot(3, n, i + n)\n",
        "    plt.imshow(x_test_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display reconstruction\n",
        "    ax = plt.subplot(3, n, i + 2 * n)\n",
        "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.gcf().set_size_inches(21, 7)    \n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHiq0xW900ut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "activations = activation_model.predict(x_test_noisy[1].reshape(1, 28, 28, 1))\n",
        "\n",
        "# let's see what the latent vector looks like for a noisy \"2\"\n",
        "display_activation(activations, 6, 6, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_OSKT2MS2cmE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}